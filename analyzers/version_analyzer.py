from pathlib import Path
from typing import List
import multiprocessing as mp
from models import FileMetrics, VersionMetrics, AggregateVersionMetrics
from reporters import CSVReporter
from utils import FileHandler, synchronized_print, OutputTarget
from .aggregate_metrics_by_tag import AggregateMetricsByTag
from .code_analyzer import CodeAnalyzer
from comparators import VersionComparator

class VersionAnalyzer:
    """Handles analysis of versions from a Git repository and local versions"""
    def __init__(self, max_processes: int = 1, include_local: bool = False, local_versions_dir: str = "./other_versions", package_name: str = "", output_dir: Path = Path(".")):
        self.package_name = package_name
        self.output_dir = output_dir
        self.code_analyzer = CodeAnalyzer()
        self.max_processes = max_processes
        self.include_local = include_local
        self.local_versions_dir = local_versions_dir
        self.entries = []
        self.repo = None
        self.comparator = VersionComparator()
    
    def analyze_versions(self) -> None:
        """Analyze all versions"""
        if not self.entries and not self.repo:
            synchronized_print(f"No versions to analyze for {self.package_name} or repository not set")
            return

        previous_aggregate_metrics = VersionMetrics()
        all_aggregate_metrics_by_tag = AggregateVersionMetrics()
        last_version = "first"
        count_versions = 0

        for i, entry in enumerate(self.entries):
            synchronized_print(f"  [{i+1}/{len(self.entries)}] Analyzing tag {entry.name}")
            try:
                if entry.source == "git":
                    self.repo.git.checkout(entry.ref.name, force=True)
                    repo_path = Path(self.repo.working_tree_dir)
                if entry.source == "local" or entry.source == "tarball":
                    repo_path = entry.ref / "package"  # entry.ref is the path to the extracted local version
                
                # curr_metrics is the list of FileMetrics for all files in the current version
                # current_metrics e.g. [[FileMetrics(package='example', version='1.0.0', file_path='index.js', ...), FileMetrics(...), ...]
                curr_metrics = self._analyze_version(entry.name, repo_path, entry.source)
                
                obfuscated_files = [f for f in curr_metrics if f.code_type == "Obfuscated"]
                if obfuscated_files:
                    synchronized_print(f"    Found {len(obfuscated_files)} obfuscated files in version {entry.name}")
                    for f in obfuscated_files:
                        path_dir = Path('deobfuscated_files') / self.package_name / entry.name
                        path_file = path_dir / f.file_path.replace('.js', '-deobfuscated.js')
                        deob = self._analyze_single_file(
                            file_path=path_file,
                            version=entry.name,
                            package_dir=path_dir,
                            source="deobfuscated"
                        )
                        curr_metrics.append(deob)
                
                synchronized_print(f"    Analyzed {len(curr_metrics)} files")
                
                # aggregate_metrics_by_tag is the aggregation of all metrics from the all files in the current version
                # aggregate_metrics_by_tag e.g. VersionMetrics(package='example', version='1.0.0', code_types=['Clear', ...], obfuscation_patterns_count=5, ...)
                aggregate_metrics_by_tag = AggregateMetricsByTag().aggregate_metrics_by_tag(curr_metrics, repo_path, entry.source)

                flags = self.comparator.compare_tags(
                    all_prev_tag_metrics=all_aggregate_metrics_by_tag,
                    prev_tag_metrics=previous_aggregate_metrics,
                    curr_tag_metrics=aggregate_metrics_by_tag,
                    package=self.package_name,
                    version_from=last_version,
                    version_to=aggregate_metrics_by_tag.version
                )

                # all_aggregate_metrics_by_tag is all aggregated metrics for all versions analyzed so far (NO last version included)
                # is updated incrementally each time, using for identifying flags and to plot the evolution of metrics over versions
                # all_aggregate_metrics_by_tag e.g. AggregateVersionMetrics(package='example', version='all up to 1.0.0 (included) + 1.1.0 (included)', code_types=['Clear', ...], obfuscation_patterns_count=15, ...)
                all_aggregate_metrics_by_tag = AggregateMetricsByTag.aggregate_metrics_incremental(all_aggregate_metrics_by_tag, aggregate_metrics_by_tag, count_versions, last_version)
                
                # Update for next iteration
                count_versions += 1
                last_version = aggregate_metrics_by_tag.version
                previous_aggregate_metrics = aggregate_metrics_by_tag

                # Incremental save detailed metrics for the current tag
                all_metrics_csv = self.output_dir / "all_metrics.csv"
                flags_csv = self.output_dir / "flags.csv"
                aggregate_metrics_csv = self.output_dir / "aggregate_metrics_by_tag.csv"
                aggregate_metrics_history_csv = self.output_dir / "aggregate_metrics_history.csv"

                CSVReporter().save_csv(all_metrics_csv, curr_metrics)
                CSVReporter().save_csv(aggregate_metrics_csv, aggregate_metrics_by_tag)
                CSVReporter().save_csv(aggregate_metrics_history_csv, all_aggregate_metrics_by_tag)
                CSVReporter().save_csv(flags_csv, flags)

            except Exception as e:
                synchronized_print(f"Error analyzing tag {entry.name}: {e}")

        return

    def _analyze_version(self, version: str, package_dir: Path, source: str) -> List[FileMetrics]:
        """Analyze all files of a specific Git version"""

        files = FileHandler().get_all_files(package_dir)
        if self.max_processes > 1:
            file_results = self._analyze_files_parallel(files, version, package_dir, source)
        else:
            file_results = self._analyze_files_sequential(files, version, package_dir, source)
        
        # Filter out None results (failed analyses)
        valid_results = [r for r in file_results if r is not None]    
        return valid_results

    def _analyze_files_sequential(self, files: List[Path], version: str, package_dir: Path, source: str) -> List[FileMetrics]:
        """Sequential analysis of files"""
        results = []
        for file_path in files:
            try:
                result = self._analyze_single_file(file_path, version, package_dir, source)
                results.append(result)
            except Exception as e:
                print(f"Error analyzing {file_path}: {e}")
                results.append(None)
        return results

    def _analyze_files_parallel(self, files: List[Path], version: str, package_dir: Path, source: str) -> List[FileMetrics]:
        """Parallel analysis of files"""
        # Prepare arguments for each file
        args_list = []
        for file_path in files:
            args_list.append((file_path, version, package_dir, source))
        
        # Use multiprocessing Pool
        with mp.Pool(processes=self.max_processes) as pool:
            results = pool.starmap(self._analyze_single_file_wrapper, args_list)
        
        return results

    def _analyze_single_file_wrapper(self, file_path: Path, version: str, package_dir: Path, source: str) -> FileMetrics:
        """Wrapper function for parallel execution that handles exceptions"""
        try:
            return self._analyze_single_file(file_path, version, package_dir, source)
        except Exception as e:
            rel_path = file_path.relative_to(package_dir) if package_dir in file_path.parents else file_path
            print(f"Error analyzing {rel_path}: {type(e).__name__}: {e}")
            return None

    def _analyze_single_file(self, file_path: Path, version: str, package_dir: Path, source: str) -> FileMetrics:
        """Analyze a single file"""
        rel_path = str(file_path.relative_to(package_dir))
        content = FileHandler().read_file(file_path)

        package_info = {
            'name': self.package_name,
            'version': version,
            'git_repo_path': str(package_dir),
            'file_name': rel_path,
            'info': source
        }

        file_metrics = self.code_analyzer.analyze_file(content, package_info)
        metrics = FileMetrics(
            package=self.package_name,
            version=version,
            file_path=rel_path,
            **file_metrics
        )
        return metrics